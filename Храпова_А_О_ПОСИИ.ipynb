{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Импорт необходимых библиотек\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Загрузка данных\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Предобработка данных\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0  # Преобразование в одномерный массив и нормализация\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Разделение на валидационную выборку\n",
        "x_val = x_train[-6000:]  # 6K для валидации\n",
        "y_val = y_train[-6000:]\n",
        "x_train = x_train[:-6000]  # Оставшиеся 54K для обучения\n",
        "y_train = y_train[:-6000]\n",
        "\n",
        "# Архитектура 1: Однослойный персептрон\n",
        "model1 = Sequential()\n",
        "model1.add(Input(shape=(784,)))\n",
        "model1.add(Dense(10, activation='relu'))\n",
        "model1.add(Dense(10, activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Оценка на тестовых данных\n",
        "_, test_acc1 = model1.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy for model 1: {test_acc1}')\n",
        "\n",
        "# Архитектура 2: Многослойный персептрон с 50 нейронами в первом слое\n",
        "model2 = Sequential()\n",
        "model2.add(Input(shape=(784,)))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Оценка на тестовых данных\n",
        "_, test_acc2 = model2.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy for model 2: {test_acc2}')\n",
        "\n",
        "# Архитектура 3: Многослойный персептрон с двумя скрытыми слоями по 50 нейронов\n",
        "model3 = Sequential()\n",
        "model3.add(Input(shape=(784,)))\n",
        "model3.add(Dense(50, activation='relu'))\n",
        "model3.add(Dense(50, activation='relu'))\n",
        "model3.add(Dense(10, activation='softmax'))\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model3.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Оценка на тестовых данных\n",
        "_, test_acc3 = model3.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy for model 3: {test_acc3}')\n",
        "\n",
        "# Подготовка данных для CNN\n",
        "x_train_cnn = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test_cnn = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Архитектура 4: Сверточная нейронная сеть (CNN)\n",
        "model4 = Sequential()\n",
        "model4.add(Input(shape=(28,28,1)))\n",
        "model4.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
        "model4.add(MaxPooling2D(pool_size=2))\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(10, activation='softmax'))\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model4.fit(x_train_cnn[:54000], y_train[:54000], epochs=10,\n",
        "           validation_data=(x_val.reshape(-1 ,28 ,28 ,1), y_val))\n",
        "\n",
        "# Оценка на тестовых данных CNN\n",
        "_, test_acc4 = model4.evaluate(x_test_cnn,y_test)\n",
        "print(f'Test accuracy for model 4: {test_acc4}')\n",
        "\n",
        "# Сравнение результатов всех моделей\n",
        "print(f'Accuracy of Model 1: {test_acc1}')\n",
        "print(f'Accuracy of Model 2: {test_acc2}')\n",
        "print(f'Accuracy of Model 3: {test_acc3}')\n",
        "print(f'Accuracy of Model 4 (CNN): {test_acc4}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebywljuRuMp7",
        "outputId": "a545bcad-505e-4a65-f769-1f3bfc1db792"
      },
      "id": "ebywljuRuMp7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6679 - loss: 1.0308 - val_accuracy: 0.8353 - val_loss: 0.4876\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8370 - loss: 0.4790 - val_accuracy: 0.8473 - val_loss: 0.4421\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8513 - loss: 0.4343 - val_accuracy: 0.8462 - val_loss: 0.4383\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8557 - loss: 0.4182 - val_accuracy: 0.8533 - val_loss: 0.4190\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8589 - loss: 0.4069 - val_accuracy: 0.8567 - val_loss: 0.4110\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8606 - loss: 0.4069 - val_accuracy: 0.8495 - val_loss: 0.4322\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8653 - loss: 0.3930 - val_accuracy: 0.8590 - val_loss: 0.4102\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8662 - loss: 0.3919 - val_accuracy: 0.8495 - val_loss: 0.4273\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8649 - loss: 0.3895 - val_accuracy: 0.8542 - val_loss: 0.4044\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8698 - loss: 0.3746 - val_accuracy: 0.8558 - val_loss: 0.4037\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8559 - loss: 0.4246\n",
            "Test accuracy for model 1: 0.8514999747276306\n",
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7542 - loss: 0.7121 - val_accuracy: 0.8528 - val_loss: 0.4184\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8548 - loss: 0.4200 - val_accuracy: 0.8645 - val_loss: 0.3761\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3697 - val_accuracy: 0.8700 - val_loss: 0.3662\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8759 - loss: 0.3442 - val_accuracy: 0.8662 - val_loss: 0.3654\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8817 - loss: 0.3254 - val_accuracy: 0.8710 - val_loss: 0.3642\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8869 - loss: 0.3081 - val_accuracy: 0.8780 - val_loss: 0.3454\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8917 - loss: 0.2985 - val_accuracy: 0.8773 - val_loss: 0.3459\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8952 - loss: 0.2857 - val_accuracy: 0.8797 - val_loss: 0.3357\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8993 - loss: 0.2783 - val_accuracy: 0.8850 - val_loss: 0.3196\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9007 - loss: 0.2739 - val_accuracy: 0.8835 - val_loss: 0.3249\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8700 - loss: 0.3571\n",
            "Test accuracy for model 2: 0.8723999857902527\n",
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.7121 - val_accuracy: 0.8357 - val_loss: 0.4545\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8525 - loss: 0.4026 - val_accuracy: 0.8603 - val_loss: 0.3793\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8706 - loss: 0.3532 - val_accuracy: 0.8642 - val_loss: 0.3619\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8799 - loss: 0.3300 - val_accuracy: 0.8633 - val_loss: 0.3751\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8841 - loss: 0.3158 - val_accuracy: 0.8715 - val_loss: 0.3658\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8880 - loss: 0.3045 - val_accuracy: 0.8763 - val_loss: 0.3538\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8917 - loss: 0.2899 - val_accuracy: 0.8795 - val_loss: 0.3442\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8963 - loss: 0.2760 - val_accuracy: 0.8793 - val_loss: 0.3369\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8996 - loss: 0.2719 - val_accuracy: 0.8815 - val_loss: 0.3431\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9029 - loss: 0.2586 - val_accuracy: 0.8805 - val_loss: 0.3546\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3617\n",
            "Test accuracy for model 3: 0.8722000122070312\n",
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.4757 - loss: 1.7445 - val_accuracy: 0.6623 - val_loss: 156.8339\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7483 - loss: 0.7100 - val_accuracy: 0.6863 - val_loss: 162.3292\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7808 - loss: 0.6117 - val_accuracy: 0.6860 - val_loss: 164.0943\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7986 - loss: 0.5655 - val_accuracy: 0.6973 - val_loss: 159.8299\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8096 - loss: 0.5402 - val_accuracy: 0.6988 - val_loss: 154.5651\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8117 - loss: 0.5223 - val_accuracy: 0.7037 - val_loss: 157.1577\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8230 - loss: 0.5036 - val_accuracy: 0.7005 - val_loss: 159.9303\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8243 - loss: 0.4942 - val_accuracy: 0.7035 - val_loss: 158.4334\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8300 - loss: 0.4838 - val_accuracy: 0.7098 - val_loss: 154.1125\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8280 - loss: 0.4823 - val_accuracy: 0.7093 - val_loss: 154.6111\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8282 - loss: 0.4965\n",
            "Test accuracy for model 4: 0.821399986743927\n",
            "Accuracy of Model 1: 0.8514999747276306\n",
            "Accuracy of Model 2: 0.8723999857902527\n",
            "Accuracy of Model 3: 0.8722000122070312\n",
            "Accuracy of Model 4 (CNN): 0.821399986743927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод\n",
        "\n",
        "Модель 1:\n",
        "Тестовая точность: 85.15%\n",
        "Модель показывает стабильные результаты, но не достигает высоких значений точности.\n",
        "\n",
        "Модель 2:\n",
        "Тестовая точность: 87.24%\n",
        "Эта модель демонстрирует заметное улучшение по сравнению с первой моделью, что может указывать на более эффективную архитектуру или гиперпараметры.\n",
        "Модель 3:\n",
        "\n",
        "Тестовая точность: 87.22%\n",
        "Результаты модели 3 немного лучше, чем у модели 2, что говорит о том, что изменения в архитектуре или параметрах могут быть полезными.\n",
        "Модель 4 (CNN):\n",
        "\n",
        "Тестовая точность: 82.14%\n",
        "Несмотря на использование сверточной нейронной сети (CNN), эта модель показала худшие результаты по сравнению с моделями 2 и 3. Это может указывать на необходимость доработки архитектуры CNN или настройки гиперпараметров\n",
        "\n",
        "Модели 2 и 3 показывают наилучшие результаты, с тестовой точностью выше 87%. Это говорит о том, что они лучше справляются с задачей классификации.\n",
        "\n",
        "Модель 4, несмотря на использование CNN, не смогла достичь конкурентоспособных результатов и требует дальнейшей оптимизации.\n"
      ],
      "metadata": {
        "id": "NYBfdlT4xbeJ"
      },
      "id": "NYBfdlT4xbeJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вариант 6**\n",
        "\n",
        "Архитектура нейронной сети\n",
        "\n",
        "Структура: 3-2-1\n",
        "\n",
        "Скорость обучения: 0.2\n",
        "\n",
        "X = {0.4;0.7;1.3}\n",
        "\n",
        "W1={0.4 -0.7; 1.2 0.6; 0.1 0.5; -1.4 0.5}\n",
        "\n",
        "W2={0.8;0.3;0.5}\n",
        "\n",
        "Y=0.7\n",
        "\n"
      ],
      "metadata": {
        "id": "m6Tx-HVayehH"
      },
      "id": "m6Tx-HVayehH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Логистическая функция активации\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Производная логистической функции активации\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Параметры сети\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Входные данные и эталонный выход\n",
        "X = np.array([[0.4], [0.7], [1.3]])\n",
        "Y = np.array([[0.7]])\n",
        "\n",
        "# Инициализация весов случайными значениями из интервала [-0.3, 0.3]\n",
        "np.random.seed(42) # Для воспроизводимости\n",
        "W1 = np.random.uniform(-0.3, 0.3, (3, 2))\n",
        "W2 = np.random.uniform(-0.3, 0.3, (2, 1))\n",
        "\n",
        "# Обучение сети\n",
        "for epoch in range(10000): # Количество эпох\n",
        "    # Прямое распространение\n",
        "    hidden_input = np.dot(X.T, W1) # Вход скрытого слоя\n",
        "    hidden_output = sigmoid(hidden_input) # Выход скрытого слоя\n",
        "\n",
        "    final_input = np.dot(hidden_output, W2) # Вход выходного слоя\n",
        "    final_output = sigmoid(final_input) # Выход сети\n",
        "\n",
        "    # Ошибка на выходе\n",
        "    error = Y - final_output\n",
        "\n",
        "    # Обратное распространение ошибки\n",
        "    d_final_output = error * sigmoid_derivative(final_output) # Ошибка на выходе\n",
        "\n",
        "    error_hidden_layer = d_final_output.dot(W2.T) # Ошибка на скрытом слое\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_output) # Ошибка на скрытом слое\n",
        "\n",
        "    # Обновление весов\n",
        "    W2 += hidden_output.T.dot(d_final_output) * learning_rate # Обновление весов выходного слоя\n",
        "    W1 += X.dot(d_hidden_layer) * learning_rate # Обновление весов скрытого слоя\n",
        "\n",
        "# Результат после обучения\n",
        "print(\"Входной вектор:\", X)\n",
        "print(\"Веса W1:\\n\", W1)\n",
        "print(\"Веса W2:\", W2)\n",
        "print(\"Выход после обучения:\", final_output)\n",
        "print(\"Эталонный выход:\", Y)\n",
        "print(\"Ошибка:\", error)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN_JYOdeuNwv",
        "outputId": "ca2425f2-4669-448b-cf35-e97b4cdbda77"
      },
      "id": "JN_JYOdeuNwv",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входной вектор: [[0.4]\n",
            " [0.7]\n",
            " [1.3]]\n",
            "Веса W1:\n",
            " [[-0.0619933   0.36661123]\n",
            " [ 0.16244097  0.22751471]\n",
            " [-0.16322027  0.1061903 ]]\n",
            "Веса W2: [[0.44140797]\n",
            " [1.05085349]]\n",
            "Выход после обучения: [[0.7]]\n",
            "Эталонный выход: [[0.7]]\n",
            "Ошибка: [[4.4408921e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Контрольные вопросы:**"
      ],
      "metadata": {
        "id": "q_2vT0cT6v_o"
      },
      "id": "q_2vT0cT6v_o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Понятие нейронной сети, архитектуры\n",
        "\n",
        "•\tНейронная сеть – вычислительная система, вдохновленная структурой и функционированием биологических нейронных сетей. Она состоит из взаимосвязанных узлов (нейронов), организованных в слои.\n",
        "\n",
        "•\tАрхитектура нейронной сети определяет структуру сети, включая количество слоев, количество нейронов в каждом слое, тип связей между нейронами и функцию активации. Существуют различные типы архитектур, такие как многослойный перцептрон (MLP), сверточные нейронные сети (CNN) и рекуррентные нейронные сети (RNN).\n"
      ],
      "metadata": {
        "id": "IlpVJUOe2HbF"
      },
      "id": "IlpVJUOe2HbF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Обучение нейронной сети\n",
        "\n",
        "•\tОбучение нейронной сети – процесс настройки весов соединений между нейронами таким образом, чтобы сеть могла выполнять определенную задачу. Обычно это делается путем предоставления сети большого количества примеров данных и использования алгоритма оптимизации (например, обратного распространения ошибки) для минимизации ошибки между предсказаниями сети и фактическими значениями.\n"
      ],
      "metadata": {
        "id": "ryOBo-jZ2bQV"
      },
      "id": "ryOBo-jZ2bQV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Основные определения: скорость обучения, эпоха, нейрон, обучающая выборка, тестовая выборка, вариационная выборка, функция активаци\n",
        "\n",
        "•\tСкорость обучения (Learning Rate) – параметр, определяющий величину шага при обновлении весов во время обучения. Слишком высокая скорость обучения может привести к нестабильности, а слишком низкая – к медленной сходимости.\n",
        "\n",
        "•\tЭпоха (Epoch) – один полный проход через весь обучающий набор данных.\n",
        "\n",
        "•\tНейрон (Neuron) – основной строительный блок нейронной сети, который принимает входные сигналы, обрабатывает их и выдает выходной сигнал.\n",
        "\n",
        "•\tОбучающая выборка (Training Set) – набор данных, используемый для обучения нейронной сети.\n",
        "\n",
        "•\tТестовая выборка (Test Set) – набор данных, используемый для оценки производительности обученной нейронной сети на новых, ранее невиданных данных.\n",
        "\n",
        "•\tВариационная выборка (Validation Set) – набор данных, используемый для настройки гиперпараметров нейронной сети и предотвращения переобучения.\n",
        "\n",
        "•\tФункция активации (Activation Function) – математическая функция, применяемая к выходу нейрона для внесения нелинейности в модель. Примеры: сигмоида, ReLU, tanh.\n"
      ],
      "metadata": {
        "id": "Qd-AuE8t2i2r"
      },
      "id": "Qd-AuE8t2i2r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Алгоритм обратного распространения ошибки\n",
        "\n",
        "Алгоритм обратного распространения ошибки (Backpropagation) – это метод обучения нейронных сетей, основанный на градиентном спуске. Он работает следующим образом:\n",
        "\n",
        "1.\tПрямое распространение: Сигнал проходит через сеть от входного слоя к выходному, вычисляя выходные значения каждого нейрона.\n",
        "\n",
        "2.\tВычисление ошибки: Рассчитывается разница между предсказанным выходом и фактическим значением.\n",
        "\n",
        "3.\tОбратное распространение: Ошибка распространяется обратно по сети, начиная с выходного слоя. На каждом слое вычисляются градиенты ошибки относительно весов соединений.\n",
        "\n",
        "4.\tОбновление весов: Веса обновляются с использованием градиентов и скорости обучения, чтобы уменьшить ошибку.\n"
      ],
      "metadata": {
        "id": "1Dbe_eL826tv"
      },
      "id": "1Dbe_eL826tv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Типы функций активации\n",
        "\n",
        "•\tСигмоида (Sigmoid): Выдает значения в диапазоне (0, 1). Часто используется в выходном слое для задач бинарной классификации.\n",
        "\n",
        "•\tReLU (Rectified Linear Unit): Выдает 0 для отрицательных\n",
        "значений и само значение для положительных. Широко используется в скрытых слоях.\n",
        "\n",
        "•\tTanh (Hyperbolic Tangent): Выдает значения в диапазоне (-1, 1).\n",
        "\n",
        "•\tSoftmax: Преобразует вектор чисел в распределение вероятностей. Часто используется в выходном слое для задач мультиклассовой классификации.\n"
      ],
      "metadata": {
        "id": "JEu8pT5h3Btg"
      },
      "id": "JEu8pT5h3Btg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Алгоритмы обучения\n",
        "\n",
        "•\tГрадиентный спуск (Gradient Descent): Базовый алгоритм оптимизации, который итеративно обновляет веса в направлении наименьшего градиента функции потерь.\n",
        "\n",
        "•\tСтохастический градиентный спуск (Stochastic Gradient Descent - SGD): Обновляет веса после обработки каждого отдельного примера данных.\n",
        "\n",
        "•\tMini-batch Gradient Descent: Обновляет веса после обработки небольшого пакета примеров данных.\n",
        "\n",
        "•\tAdam: Адаптивный алгоритм оптимизации, который сочетает преимущества SGD и Momentum.\n",
        "\n",
        "•\tRMSprop: Еще один адаптивный алгоритм оптимизации, который использует экспоненциально взвешенное скользящее среднее квадратов градиентов.\n"
      ],
      "metadata": {
        "id": "5nAdORV13ITV"
      },
      "id": "5nAdORV13ITV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}